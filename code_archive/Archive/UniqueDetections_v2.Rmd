---
title: "R Notebook"
output: html_notebook
---

Questions: If we're going to be deleting duplicates, which days values would we use for things like fire radiative power? 
Do we only care about the date of ignition? Do we at all care about how long there was something being detected there for? 


From MODIS Active Fire Products Collection 6 guide:   
*8.7.3 Are persistent hot spots filtered out of the fire location product?*
No. Unlike the CMG fire product, static, persistent hot spots are not removed from the MCD14ML product. For Collection 6, you can use the new type attribute to identify such fire pixels in the MCD14ML product.

*8.3.9 I want to estimate burned area using active fire data. What effective area burned should I assume for each fire pixel?*
Pulling this off to an acceptable degree of accuracy is generally not possible due to nontrivial spatial and temporal sampling issues. For some applications, however, acceptable accuracy can be achieved, although the effective area burned per fire pixel is not simply a constant, but rather varies with respect to several different vegetation- and fire-related variables. See Giglio et al. (2006b) for more information.



Read in the necessary libraries, and some code to read in the fire detections that have already been cut down to just the area within the buffer, and make the columns be the correct class.
```{r}
# the libraries
library(sp)
library(GISTools)
library(rgdal)
library(plyr)

## create a mini function that makes the date of the detected fire be read in as a POSIXct date, rather than as a character or a string
setClass('yyyymmdd')
setAs("character","yyyymmdd", function(from) as.Date(from, format="%Y%m%d"))

## using POSIXct would work too, though this incorporates time zones (which could be good or bad!)
# setClass('yyyymmdd')
# setAs("character","yyyymmdd", function(from) as.POSIXct(from, format="%Y%m%d"))

# read in the data
detections_500km<-read.table("/Users/laurenhendricks/Documents/Borneo/Ketapang_ClimateFire/mcd14ml_KTG_500km.txt",sep=" ",header = T,colClasses = c("yyyymmdd","character","factor","numeric","numeric","numeric","numeric","integer","numeric","integer","factor"))

# pull out only the year. Note that this way of doing it gets teh year as a character (so it's no longer a real date) but this makes it possible for use to make a better histogram
detections_500km$year<-format(detections_500km$YYYYMMDD,"%Y")

# doing it this way instead of using the histogram function to make slightly prettier barplot
#barplot(table(detections_500km$year),main="Number of Fire Detections: 500 km",xlab="Year",ylab="",las=2)
```

Figure out how to detect duplicate detections, based on latitude and longitude of the detection. 
```{r}
# tells you how many unique detections there are
length(unique(detections_500km$lat))    # 5929 -- ~15% of total
length(unique(detections_500km$lon))    # 7678 -- ~20% of total

# but it isn't helpful to only have lat OR long, you really have to have both (because things could have the same lat but different longs)
detections_500km_nodups<-detections_500km[which(duplicated(detections_500km[,c('lat','lon')])==F),]
```

Here we assign an unique number to each unique set of lat/long values. 
```{r}
# now try it with full data set
#detections_500km_IDs<-transform(detections_500km,CLUSTER_ID=as.numeric(interaction(detections_500km$lat,detections_500km$lon, drop=T)))
# works but takes a long time --> so don't want to do this with the whole data set often!!!!! 

# how many unique clusters are there? count the number of unique values
print(paste(length(unique(detections_500km_IDs$CLUSTER_ID)),"unique clusters"))

# write the data to a file
write.csv(detections_500km_IDs,file="/Users/laurenhendricks/Documents/Borneo/Ketapang_ClimateFire/mcd14ml_KTG_500km_ClusterID.txt",row.names=F)

detections_500km<-read.csv(file="/Users/laurenhendricks/Documents/Borneo/Ketapang_ClimateFire/mcd14ml_KTG_500km_ClusterID.txt")


```

```{r}
# remove the raw data; we only need the one with cluster IDs
rm(detections_500km)

# then rename the one with the cluster IDs as detections_500km
detections_500km<-detections_500km_IDs

# and remove the one with the IDs
rm(detections_500km_IDs)
```

```{r}
# read in the data
#detections_500km2<-read.csv(file="/Users/laurenhendricks/Documents/Borneo/Ketapang_ClimateFire/mcd14ml_KTG_500km_ClusterID.txt")     # row.names=1 tells the function that the row names are in the first column (the row names are just the number from when it was written in, but now it's too late to get them out)

#detections_500km$YYYYMMDD<-as.Date(detections_500km$YYYYMMDD,format="%Y-%m-%d")

# then make the data spatial
detections_500km_sp<-SpatialPointsDataFrame(cbind(detections_500km$lon,detections_500km$lat),detections_500km,proj4string = CRS("+proj=longlat +ellps=WGS84"))
                                          
# project the data                                          
detections_500km_sp_proj<-spTransform(detections_500km_sp,"+proj=utm +zone=49 +south +a=6378160 +b=6356774.50408554 +units=m +no_defs")
```

To make this run faster while we're just testing things out, subset down to a smaller radius around Ketapang. First, make a shapefile with the Ketapang airport (KTG)
```{r}
# create a shapefile with the coordinates of KTG
KTG<-cbind(109.9619,-1.8166)
KTG.sp<-SpatialPoints(KTG,proj4string = CRS("+proj=longlat +ellps=WGS84"),bbox=NULL)
KTG.sp_proj<-spTransform(KTG.sp,"+proj=utm +zone=49 +south +a=6378160 +b=6356774.50408554 +units=m +no_defs")
```

Now, make some buffers. Remember that gBuffer needs projected data! Also that the units of this projection are meters. 
```{r}
# buffers of varying distances. projection units are in meters! 
KTG_buffer5km<-gBuffer(KTG.sp_proj,width=5000,byid=FALSE)
KTG_buffer50km<-gBuffer(KTG.sp_proj,width=50000,byid=FALSE)
KTG_buffer250km<-gBuffer(KTG.sp_proj,width=250000,byid=FALSE)

# then plot all of the buffers to make sure that it all works
plot(KTG_buffer250km,col="blue")
plot(KTG_buffer50km,add=T,col="green")
plot(KTG.sp_proj,pch=17,add=T)

# cut out only the things in the buffer
detections_250km_c<-detections_500km_sp_proj[KTG_buffer250km,]
detections_50km_c<-detections_500km_sp_proj[KTG_buffer50km,]
detections_5km_c<-detections_500km_sp_proj[KTG_buffer5km,]

# then get just the data frames
detections_250km<-as.data.frame(detections_250km_c)
detections_50km<-as.data.frame(detections_50km_c)
detections_5km<-as.data.frame(detections_5km_c)
```

Remove the spatial data because we don't need it anymore. (Not absolutely necessary but cleans up the environment). 
```{r}
rm(detections_500km_sp,detections_500km_sp_proj)
rm(detections_250km_c,detections_50km_c,detections_5km_c)
rm(KTG_buffer250km,KTG_buffer50km,KTG_buffer5km)
rm(KTG.sp,KTG.sp_proj)
```

Now, play with the 50km radius data, because that's only ~8000 records. 
```{r}
length(unique(detections_50km$CLUSTER_ID))
# there are 7478 unique fires; so there are 462 duplicates
```

Then do some summary statistics by day. 
```{r}
# use count (from the plyr package) to see how many ignitions there are on a given day
ignitions_byday<-count(detections_50km,vars="YYYYMMDD")
# REMEMBER THAT WE HAVEN'T REMOVED DUPLICATES YET!!!!!!!
# this runs pretty quickly!!! 

# look at this graphically
plot(t$YYYYMMDD,t$freq,type="l")

# start to work on how to figure out the diplicates
ignitions_bycluster<-count(detections_50km,vars="CLUSTER_ID")

# join the data
detections_50km_joined<-join(detections_50km,ignitions_bycluster,by="CLUSTER_ID",type="left",match="all")

# subset out entries that are totally unique (frequency = 1, so no duplicate lat/longs)
detections_50km_nodups<-detections_50km_joined[detections_50km_joined$freq==1,]

# then subset out the entries that are duplicates
detections_50km_dups<-detections_50km_joined[detections_50km_joined$freq>1,]
```

Try getting a total FRP per day. (Note that this actually runs pretty fast, even for the 250 km data set.)
```{r}
# first for the 5km data set
FRP_byday_5km<-aggregate(detections_5km$FRP,by=list(detections_5km$YYYYMMDD),sum)
colnames(FRP_byday_5km)<-c("Date","Total_FRP")

# then for the 50 km data set (795 days)
FRP_byday_50km<-aggregate(detections_50km$FRP,by=list(detections_50km$YYYYMMDD),sum)
colnames(FRP_byday_50km)<-c("Date","Total_FRP")

# then for the 250 km data set (2709 days)
FRP_byday_250km<-aggregate(detections_250km$FRP,by=list(detections_250km$YYYYMMDD),sum)
colnames(FRP_byday_250km)<-c("Date","Total_FRP")

# look at the data
plot(FRP_byday_250km$Total_FRP~FRP_byday_250km$Date)
```

