---
title: "Fire Detections using MODIS Active Fire Products"
output: html_notebook
---
Read in the necessary libraries, and some code to read in the fire detections that have already been cut down to just the area within the buffer, and make the columns be the correct class.
```{r}
# load libraries
# install the package if you don't already have it
if (!require("sp")) {
     install.packages("sp")
     library(sp)
     }

if (!require("GISTools")) {
     install.packages("GISTools")
     library(GISTools)
     }

if (!require("rgdal")) {
     install.packages("rgdal")
     library(rgdal)
     }

if (!require("plyr")) {
     install.packages("plyr")
     library(plyr)
     }

## create a mini function that makes the date of the detected fire be read in as a POSIXct date, rather than as a character or a string
setClass('yyyymmdd')
setAs("character","yyyymmdd", function(from) as.Date(from, format="%Y%m%d"))

setClass('yyyy-mm-dd')
setAs("character","yyyy-mm-dd", function(from) as.Date(from, format="%Y-%m-%d"))
```

```{r}
# read in the data--which has already been selected out to be just the radius we want. 
detections_500km_raw<-read.table("/Users/laurenhendricks/Documents/Borneo/Ketapang_ClimateFire/mcd14ml_KTG_500km.txt",sep=" ",header = T,colClasses = c("yyyymmdd","character","factor","numeric","numeric","numeric","numeric","integer","numeric","integer","factor"))

# pull out only the year. Note that this way of doing it gets the year as a character (so it's no longer a real date) but this makes it possible for use to make a better histogram
detections_500km_raw$year<-format(detections_500km_raw$YYYYMMDD,"%Y")

# doing it this way instead of using the histogram function to make slightly prettier barplot
barplot(table(detections_500km_raw$year),main="Number of Fire Detections: 500 km",xlab="Year",ylab="",las=2)

# how many unique detections are there based on only lat or only long?
length(unique(detections_500km_raw$lat))    # 5929 -- ~15% of total
length(unique(detections_500km_raw$lon))    # 7678 -- ~20% of total

# we won't need this file anymore so we can remove it from the memory
rm(detections_500km_raw)
```

So, one strategy for figuring out possible duplicate detections (detecting the same fire that is still burning on the next pass) is that we can assign an unique number to each unique set of lat/long values. 
NOTE: ONLY RUN THIS CHUNK IF YOU NEED TO RE-CLUSTER THE DATA. IT IS VERY SLOW FOR THE FULL 500 KM RADIUS RECORD (IE 12+ HOURS) SO JUST READ IN THE DATA FROM WHEN THIS WAS DONE BEFORE INSTEAD
```{r}
# now try it with full data set
#detections_500km_IDs<-transform(detections_500km,CLUSTER_ID=as.numeric(interaction(detections_500km$lat,detections_500km$lon, drop=T)))
# works but takes a long time --> so don't want to do this with the whole data set often!!!!! 

# how many unique clusters are there? count the number of unique values
#print(paste(length(unique(detections_500km_IDs$CLUSTER_ID)),"unique clusters"))

# write the data to a file
#write.csv(detections_500km_IDs,file="/Users/laurenhendricks/Documents/Borneo/Ketapang_ClimateFire/mcd14ml_KTG_500km_ClusterID.txt",row.names=F)
```

Instead we can read in the data with the unique number (cluster ID) already added! 
```{r}
# read in the data that has the cluster ID added
detections_500km<-read.csv(file="/Users/laurenhendricks/Documents/Borneo/Ketapang_ClimateFire/mcd14ml_KTG_500km_ClusterID.txt",sep=",",header = T,colClasses = c("yyyy-mm-dd","character","factor",rep("numeric",6),"integer",rep("factor",2),"numeric"))

# then make the data spatial
detections_500km_sp<-SpatialPointsDataFrame(cbind(detections_500km$lon,detections_500km$lat),detections_500km,proj4string = CRS("+proj=longlat +ellps=WGS84"))
                                          
# project the data                                          
detections_500km_sp_proj<-spTransform(detections_500km_sp,"+proj=utm +zone=49 +south +a=6378160 +b=6356774.50408554 +units=m +no_defs")
```

To make this run faster while we're just testing things out, subset down to a smaller radius around Ketapang. First, make a shapefile with the Ketapang airport (KTG)
```{r}
# create a shapefile with the coordinates of KTG
KTG<-cbind(109.9619,-1.8166)
KTG.sp<-SpatialPoints(KTG,proj4string = CRS("+proj=longlat +ellps=WGS84"),bbox=NULL)
KTG.sp_proj<-spTransform(KTG.sp,"+proj=utm +zone=49 +south +a=6378160 +b=6356774.50408554 +units=m +no_defs")
```

Now, make some buffers. Remember that gBuffer needs projected data! Also that the units of this projection are meters. 
```{r}
# buffers of varying distances. projection units are in meters! 
KTG_buffer5km<-gBuffer(KTG.sp_proj,width=5000,byid=FALSE)
KTG_buffer50km<-gBuffer(KTG.sp_proj,width=50000,byid=FALSE)
KTG_buffer250km<-gBuffer(KTG.sp_proj,width=250000,byid=FALSE)

# then plot all of the buffers to make sure that it all works
plot(KTG_buffer250km,col="blue")
plot(KTG_buffer50km,add=T,col="green")
plot(KTG_buffer5km,add=T,col="yellow")
plot(KTG.sp_proj,pch=16,cex=0.25,add=T)

# cut out only the things in the buffer
detections_250km_c<-detections_500km_sp_proj[KTG_buffer250km,]
detections_50km_c<-detections_500km_sp_proj[KTG_buffer50km,]
detections_5km_c<-detections_500km_sp_proj[KTG_buffer5km,]

# then get just the data frames
detections_250km<-as.data.frame(detections_250km_c)
detections_50km<-as.data.frame(detections_50km_c)
detections_5km<-as.data.frame(detections_5km_c)
```

Remove the spatial data because we don't need it anymore. (Not absolutely necessary but cleans up the environment). 
```{r}
rm(detections_500km_sp,detections_500km_sp_proj)
rm(detections_250km_c,detections_50km_c,detections_5km_c)
rm(KTG_buffer250km,KTG_buffer50km,KTG_buffer5km)
rm(KTG.sp,KTG.sp_proj)
```

Now, play with the 50km radius data, because that's only ~8000 records. 
```{r}
length(unique(detections_50km$CLUSTER_ID))
# there are 7478 unique fires; so there are 462 duplicates
```

Then do some summary statistics by day. REMEMBER THAT WE HAVEN'T REMOVED DUPLICATES YET!!!!!!!
```{r}
######## 5 km
# use count (from the plyr package) to see how many ignitions there are on a given day
ignitions_byday_5km<-count(detections_5km,vars="YYYYMMDD")
colnames(ignitions_byday_5km)<-c("Date","Total_Ignitions")

# look at this graphically
plot(ignitions_byday_5km$Date,ignitions_byday_5km$Total_Ignitions,type="s",xlab="Date",ylab="Number of Ignitions",main="5 km radius")

######## 50 km
# use count (from the plyr package) to see how many ignitions there are on a given day
ignitions_byday_50km<-count(detections_50km,vars="YYYYMMDD")
colnames(ignitions_byday_50km)<-c("Date","Total_Ignitions")
# this runs pretty quickly!!! 

# look at this graphically
plot(ignitions_byday_50km$Date,ignitions_byday_50km$Total_Ignitions,type="l",xlab="Date",ylab="Number of Ignitions",main="50 km radius")

######## 250 km
# use count (from the plyr package) to see how many ignitions there are on a given day
ignitions_byday_250km<-count(detections_250km,vars="YYYYMMDD")
colnames(ignitions_byday_250km)<-c("Date","Total_Ignitions")

# look at this graphically
plot(ignitions_byday_250km$Date,ignitions_byday_250km$Total_Ignitions,type="l",xlab="Date",ylab="Number of Ignitions",main="250 km radius")


######## 500 km
# use count (from the plyr package) to see how many ignitions there are on a given day
ignitions_byday_500km<-count(detections_500km,vars="YYYYMMDD")
colnames(ignitions_byday_500km)<-c("Date","Total_Ignitions")
# this runs pretty quickly!!! 

# look at this graphically
plot(ignitions_byday_500km$Date,ignitions_byday_500km$Total_Ignitions,type="l",xlab="Date",ylab="Number of Ignitions",main="500 km radius")
```

Then calcluate a total FRP per day. Here, removing duplicates wouldn't make sense so it should be okay to stick with this how it is. (Note that this actually runs pretty fast, even for the 250 km data set.)
```{r}
######## 5 km
# calcluate sum of FRP for each day, first for the 5km data set
FRP_byday_5km<-aggregate(detections_5km$FRP,by=list(detections_5km$YYYYMMDD),sum)
colnames(FRP_byday_5km)<-c("Date","Total_FRP")

# look at the data
plot(FRP_byday_5km$Total_FRP~FRP_byday_5km$Date,type="s",xlab="Date",ylab="Total Fire Radiative Power",main="5 km radius")

######## 50 km
# then for the 50 km data set (795 days)
FRP_byday_50km<-aggregate(detections_50km$FRP,by=list(detections_50km$YYYYMMDD),sum)
colnames(FRP_byday_50km)<-c("Date","Total_FRP")

# look at the data
plot(FRP_byday_50km$Total_FRP~FRP_byday_50km$Date,type="s",xlab="Date",ylab="Total Fire Radiative Power",main="50 km radius")

######## 250 km
# then for the 250 km data set (2709 days)
FRP_byday_250km<-aggregate(detections_250km$FRP,by=list(detections_250km$YYYYMMDD),sum)
colnames(FRP_byday_250km)<-c("Date","Total_FRP")

# look at the data
plot(FRP_byday_250km$Total_FRP~FRP_byday_250km$Date,type="l",xlab="Date",ylab="Total Fire Radiative Power",main="250 km radius")

######## 500 km
# then for the 500 km data set 
FRP_byday_500km<-aggregate(detections_500km$FRP,by=list(detections_500km$YYYYMMDD),sum)
colnames(FRP_byday_500km)<-c("Date","Total_FRP")

# look at the data
plot(FRP_byday_500km$Total_FRP~FRP_byday_500km$Date,type="l",xlab="Date",ylab="Total Fire Radiative Power",main="500 km radius")
```

Also write out all of the daily FRP data so we can combine it with the weather data. 
```{r}
write.csv(FRP_byday_500km,file="/Users/laurenhendricks/Documents/Borneo/Ketapang_ClimateFire/FRP_daily_500km.txt",row.names=FALSE)
write.csv(FRP_byday_250km,file="/Users/laurenhendricks/Documents/Borneo/Ketapang_ClimateFire/FRP_daily_250km.txt",row.names=FALSE)
write.csv(FRP_byday_50km,file="/Users/laurenhendricks/Documents/Borneo/Ketapang_ClimateFire/FRP_daily_50km.txt",row.names=FALSE)
write.csv(FRP_byday_5km,file="/Users/laurenhendricks/Documents/Borneo/Ketapang_ClimateFire/FRP_daily_5km.txt",row.names=FALSE)
```


NOW, figure out which ones are duplicates, and which are not duplicates. First determine the max number of ignitions in a cluster (for the smaller radii it will be the number for 500 km or less, so we only need to do this once).
```{r}
# calculate how many ignitions there are in each cluster
ignitions_bycluster_500km<-count(detections_500km,vars="CLUSTER_ID")

# how many is the max number in a cluster? 
print(paste(c("At most there are",max(ignitions_bycluster_500km$freq),"ignitions in a cluster.")))
```
So this tells us that we need to deal with at most 5 in a cluster.  

Now, start detecting the duplicates. 
```{r}
# calculate how many ignitions there are in each cluster
ignitions_bycluster_50km<-count(detections_50km,vars="CLUSTER_ID")

# then join the real data with the number of ignitions in each cluster (so this is a one-to-many-type merge)
joined_detections_50km<-join(detections_50km,ignitions_bycluster_50km,by="CLUSTER_ID",type="left",match="all")

# make a column to store info in--will use this later when we identify the unique and not unique detections
joined_detections_50km$Duplicate<-NA

# subset out entries that are totally unique (frequency = 1, so no duplicate lat/longs) because there is no need to go through them again in a for loop
unique_detections_50km<-joined_detections_50km[joined_detections_50km$freq==1,]
unique_detections_50km$Duplicate<-"Unique"   # we know that they're all unique because that was our criteria for dividing them up! 

# then subset out the entries that are duplicates
duplicate_detections_50km<-joined_detections_50km[joined_detections_50km$freq>1,]

# and then put them in order by cluster ID AND by date so the for loop will work. THIS IS REALLY IMPORTANT!
ordered_duplicate_detections_50km<-duplicate_detections_50km[order(duplicate_detections_50km$CLUSTER_ID,duplicate_detections_50km$YYYYMMDD),]

# set the threshold for uniqueness (number of days)
threshold<-20

# counter for the for loop
i<-1

# then start a for loop
# loop through however many clusters there are
for(cluster in seq(1:length(unique(ordered_duplicate_detections_50km$CLUSTER_ID)))){
     # get the number of detections in the cluster     
     num_in_cluster<-ordered_duplicate_detections_50km$freq[i]
     # and then what happens depends on how many there are in the cluster
     if(num_in_cluster==2){
          # get the difference between the day the different detections happened on
          difference<-ordered_duplicate_detections_50km$YYYYMMDD[i+1]-ordered_duplicate_detections_50km$YYYYMMDD[i]
          # then, depending on the difference, either label them as unique or not unique
          if(difference>threshold){
               ordered_duplicate_detections_50km$Duplicate[i]<-"Unique"
               ordered_duplicate_detections_50km$Duplicate[i+1]<-"Unique"
          } else {ordered_duplicate_detections_50km$Duplicate[i]<-"Not Unique"
          ordered_duplicate_detections_50km$Duplicate[i+1]<-"Not Unique"
          }
     } else if(num_in_cluster==3){
          # get the difference between the day the different detections happened on for each CONSECUTIVE pair
          difference1<-ordered_duplicate_detections_50km$YYYYMMDD[i+1]-ordered_duplicate_detections_50km$YYYYMMDD[i]
          difference2<-ordered_duplicate_detections_50km$YYYYMMDD[i+2]-ordered_duplicate_detections_50km$YYYYMMDD[i+1]
          
          # then, depending on the difference, either label them as unique or not unique
          if(difference1>threshold){
               ordered_duplicate_detections_50km$Duplicate[i]<-"Unique"
               ordered_duplicate_detections_50km$Duplicate[i+1]<-"Unique"
          } else {ordered_duplicate_detections_50km$Duplicate[i]<-"Not Unique"
          ordered_duplicate_detections_50km$Duplicate[i+1]<-"Not Unique"
          }
          if(difference2>threshold){
               # don't re-write what is in [i+1] with unique, because we don't want to lose the info if it wasn't unique from [i]
               ordered_duplicate_detections_50km$Duplicate[i+2]<-"Unique"
          } else {ordered_duplicate_detections_50km$Duplicate[i+1]<-"Not Unique"
          ordered_duplicate_detections_50km$Duplicate[i+2]<-"Not Unique"
          }
     } else if(num_in_cluster==4){
          # get the difference between the day the different detections happened on for each CONSECUTIVE pair
          difference1<-ordered_duplicate_detections_50km$YYYYMMDD[i+1]-ordered_duplicate_detections_50km$YYYYMMDD[i]
          difference2<-ordered_duplicate_detections_50km$YYYYMMDD[i+2]-ordered_duplicate_detections_50km$YYYYMMDD[i+1]
          difference3<-ordered_duplicate_detections_50km$YYYYMMDD[i+3]-ordered_duplicate_detections_50km$YYYYMMDD[i+3]
          
          # then, depending on the difference, either label them as unique or not unique
          if(difference1>threshold){
               ordered_duplicate_detections_50km$Duplicate[i]<-"Unique"
               ordered_duplicate_detections_50km$Duplicate[i+1]<-"Unique"
          } else {ordered_duplicate_detections_50km$Duplicate[i]<-"Not Unique"
          ordered_duplicate_detections_50km$Duplicate[i+1]<-"Not Unique"
          }
          if(difference2>threshold){
               # don't re-write what is in [i+1] with unique, because we don't want to lose the info if it wasn't unique from [i]
               ordered_duplicate_detections_50km$Duplicate[i+2]<-"Unique"
          } else {ordered_duplicate_detections_50km$Duplicate[i+1]<-"Not Unique"
          ordered_duplicate_detections_50km$Duplicate[i+2]<-"Not Unique"
          }
          if(difference3>threshold){
               # don't re-write what is in [i+2] with unique, because we don't want to lose the info if it wasn't unique from [i+1]
               ordered_duplicate_detections_50km$Duplicate[i+3]<-"Unique"
          } else {ordered_duplicate_detections_50km$Duplicate[i+2]<-"Not Unique"
          ordered_duplicate_detections_50km$Duplicate[i+3]<-"Not Unique"
          }
     } else if(num_in_cluster==5){
          difference1<-ordered_duplicate_detections_50km$YYYYMMDD[i+1]-ordered_duplicate_detections_50km$YYYYMMDD[i]
          difference2<-ordered_duplicate_detections_50km$YYYYMMDD[i+2]-ordered_duplicate_detections_50km$YYYYMMDD[i+1]
          difference3<-ordered_duplicate_detections_50km$YYYYMMDD[i+3]-ordered_duplicate_detections_50km$YYYYMMDD[i+3]
          difference4<-ordered_duplicate_detections_50km$YYYYMMDD[i+4]-ordered_duplicate_detections_50km$YYYYMMDD[i+4]
          
          # then, depending on the difference, either label them as unique or not unique
          if(difference1>threshold){
               ordered_duplicate_detections_50km$Duplicate[i]<-"Unique"
               ordered_duplicate_detections_50km$Duplicate[i+1]<-"Unique"
          } else {ordered_duplicate_detections_50km$Duplicate[i]<-"Not Unique"
          ordered_duplicate_detections_50km$Duplicate[i+1]<-"Not Unique"
          }
          if(difference2>threshold){
               # don't re-write what is in [i+1] with unique, because we don't want to lose the info if it wasn't unique from [i]
               ordered_duplicate_detections_50km$Duplicate[i+2]<-"Unique"
          } else {ordered_duplicate_detections_50km$Duplicate[i+1]<-"Not Unique"
          ordered_duplicate_detections_50km$Duplicate[i+2]<-"Not Unique"
          }
          if(difference3>threshold){
               # don't re-write what is in [i+2] with unique, because we don't want to lose the info if it wasn't unique from [i+1]
               ordered_duplicate_detections_50km$Duplicate[i+3]<-"Unique"
          } else {ordered_duplicate_detections_50km$Duplicate[i+2]<-"Not Unique"
          ordered_duplicate_detections_50km$Duplicate[i+3]<-"Not Unique"
          }
          if(difference4>threshold){
               # don't re-write what is in [i+3] with unique, because we don't want to lose the info if it wasn't unique from [i+2]
               ordered_duplicate_detections_50km$Duplicate[i+4]<-"Unique"
          } else {ordered_duplicate_detections_50km$Duplicate[i+3]<-"Not Unique"
          ordered_duplicate_detections_50km$Duplicate[i+4]<-"Not Unique"
          }
     }
     
     # then we need to advance the counter and go on to the next cluster
     i<-i+num_in_cluster
}

# last, recombine the unique and not unique data frames
combined_detections_50km<-rbind.data.frame(ordered_duplicate_detections_50km,unique_detections_50km)

# and clean up a bit
rm(ordered_duplicate_detections_50km,duplicate_detections_50km,unique_detections_50km,joined_detections_50km)
# then we can decide what to do with the duplicates

# but first write out to a csv
write.csv(combined_detections_50km,file="/Users/laurenhendricks/Documents/Borneo/Ketapang_ClimateFire/mcd14ml_KTG_500km_dupsID.txt")
```


Do the duplicate detections task for the data frame with everything within 500 km of KTG. 
```{r}
# calculate how many ignitions there are in each cluster
ignitions_bycluster_5000km<-count(detections_500km,vars="CLUSTER_ID")

# then join the real data with the number of ignitions in each cluster (so this is a one-to-many-type merge)
joined_detections_500km<-join(detections_500km,ignitions_bycluster_500km,by="CLUSTER_ID",type="left",match="all")

# make a column to store info in--will use this later when we identify the unique and not unique detections
joined_detections_500km$Duplicate<-NA

# subset out entries that are totally unique (frequency = 1, so no duplicate lat/longs) because there is no need to go through them again in a for loop
unique_detections_500km<-joined_detections_500km[joined_detections_500km$freq==1,]
unique_detections_500km$Duplicate<-"Unique"   # we know that they're all unique because that was our criteria for dividing them up! 

# then subset out the entries that are duplicates
duplicate_detections_500km<-joined_detections_500km[joined_detections_500km$freq>1,]

# and then put them in order by cluster ID AND by date so the for loop will work. THIS IS REALLY IMPORTANT!
ordered_duplicate_detections_500km<-duplicate_detections_500km[order(duplicate_detections_500km$CLUSTER_ID,duplicate_detections_500km$YYYYMMDD),]

# set the threshold for uniqueness (number of days)
threshold<-20

# counter for the for loop
i<-1

# then start a for loop
# loop through however many clusters there are
for(cluster in seq(1:length(unique(ordered_duplicate_detections_500km$CLUSTER_ID)))){
     # get the number of detections in the cluster     
     num_in_cluster<-ordered_duplicate_detections_500km$freq[i]
     # and then what happens depends on how many there are in the cluster
     if(num_in_cluster==2){
          # get the difference between the day the different detections happened on
          difference<-ordered_duplicate_detections_500km$YYYYMMDD[i+1]-ordered_duplicate_detections_500km$YYYYMMDD[i]
          # then, depending on the difference, either label them as unique or not unique
          if(difference>threshold){
               ordered_duplicate_detections_500km$Duplicate[i]<-"Unique"
               ordered_duplicate_detections_500km$Duplicate[i+1]<-"Unique"
          } else {ordered_duplicate_detections_500km$Duplicate[i]<-"Not Unique"
          ordered_duplicate_detections_500km$Duplicate[i+1]<-"Not Unique"
          }
     } else if(num_in_cluster==3){
          # get the difference between the day the different detections happened on for each CONSECUTIVE pair
          difference1<-ordered_duplicate_detections_500km$YYYYMMDD[i+1]-ordered_duplicate_detections_500km$YYYYMMDD[i]
          difference2<-ordered_duplicate_detections_500km$YYYYMMDD[i+2]-ordered_duplicate_detections_500km$YYYYMMDD[i+1]
          
          # then, depending on the difference, either label them as unique or not unique
          if(difference1>threshold){
               ordered_duplicate_detections_500km$Duplicate[i]<-"Unique"
               ordered_duplicate_detections_500km$Duplicate[i+1]<-"Unique"
          } else {ordered_duplicate_detections_500km$Duplicate[i]<-"Not Unique"
          ordered_duplicate_detections_500km$Duplicate[i+1]<-"Not Unique"
          }
          if(difference2>threshold){
               # don't re-write what is in [i+1] with unique, because we don't want to lose the info if it wasn't unique from [i]
               ordered_duplicate_detections_500km$Duplicate[i+2]<-"Unique"
          } else {ordered_duplicate_detections_500km$Duplicate[i+1]<-"Not Unique"
          ordered_duplicate_detections_500km$Duplicate[i+2]<-"Not Unique"
          }
     } else if(num_in_cluster==4){
          # get the difference between the day the different detections happened on for each CONSECUTIVE pair
          difference1<-ordered_duplicate_detections_500km$YYYYMMDD[i+1]-ordered_duplicate_detections_500km$YYYYMMDD[i]
          difference2<-ordered_duplicate_detections_500km$YYYYMMDD[i+2]-ordered_duplicate_detections_500km$YYYYMMDD[i+1]
          difference3<-ordered_duplicate_detections_500km$YYYYMMDD[i+3]-ordered_duplicate_detections_500km$YYYYMMDD[i+3]
          
          # then, depending on the difference, either label them as unique or not unique
          if(difference1>threshold){
               ordered_duplicate_detections_500km$Duplicate[i]<-"Unique"
               ordered_duplicate_detections_500km$Duplicate[i+1]<-"Unique"
          } else {ordered_duplicate_detections_500km$Duplicate[i]<-"Not Unique"
          ordered_duplicate_detections_500km$Duplicate[i+1]<-"Not Unique"
          }
          if(difference2>threshold){
               # don't re-write what is in [i+1] with unique, because we don't want to lose the info if it wasn't unique from [i]
               ordered_duplicate_detections_500km$Duplicate[i+2]<-"Unique"
          } else {ordered_duplicate_detections_500km$Duplicate[i+1]<-"Not Unique"
          ordered_duplicate_detections_500km$Duplicate[i+2]<-"Not Unique"
          }
          if(difference3>threshold){
               # don't re-write what is in [i+2] with unique, because we don't want to lose the info if it wasn't unique from [i+1]
               ordered_duplicate_detections_500km$Duplicate[i+3]<-"Unique"
          } else {ordered_duplicate_detections_500km$Duplicate[i+2]<-"Not Unique"
          ordered_duplicate_detections_500km$Duplicate[i+3]<-"Not Unique"
          }
     } else if(num_in_cluster==5){
          difference1<-ordered_duplicate_detections_500km$YYYYMMDD[i+1]-ordered_duplicate_detections_500km$YYYYMMDD[i]
          difference2<-ordered_duplicate_detections_500km$YYYYMMDD[i+2]-ordered_duplicate_detections_500km$YYYYMMDD[i+1]
          difference3<-ordered_duplicate_detections_500km$YYYYMMDD[i+3]-ordered_duplicate_detections_500km$YYYYMMDD[i+3]
          difference4<-ordered_duplicate_detections_500km$YYYYMMDD[i+4]-ordered_duplicate_detections_500km$YYYYMMDD[i+4]
          
          # then, depending on the difference, either label them as unique or not unique
          if(difference1>threshold){
               ordered_duplicate_detections_500km$Duplicate[i]<-"Unique"
               ordered_duplicate_detections_500km$Duplicate[i+1]<-"Unique"
          } else {ordered_duplicate_detections_500km$Duplicate[i]<-"Not Unique"
          ordered_duplicate_detections_500km$Duplicate[i+1]<-"Not Unique"
          }
          if(difference2>threshold){
               # don't re-write what is in [i+1] with unique, because we don't want to lose the info if it wasn't unique from [i]
               ordered_duplicate_detections_500km$Duplicate[i+2]<-"Unique"
          } else {ordered_duplicate_detections_500km$Duplicate[i+1]<-"Not Unique"
          ordered_duplicate_detections_500km$Duplicate[i+2]<-"Not Unique"
          }
          if(difference3>threshold){
               # don't re-write what is in [i+2] with unique, because we don't want to lose the info if it wasn't unique from [i+1]
               ordered_duplicate_detections_500km$Duplicate[i+3]<-"Unique"
          } else {ordered_duplicate_detections_500km$Duplicate[i+2]<-"Not Unique"
          ordered_duplicate_detections_500km$Duplicate[i+3]<-"Not Unique"
          }
          if(difference4>threshold){
               # don't re-write what is in [i+3] with unique, because we don't want to lose the info if it wasn't unique from [i+2]
               ordered_duplicate_detections_500km$Duplicate[i+4]<-"Unique"
          } else {ordered_duplicate_detections_500km$Duplicate[i+3]<-"Not Unique"
          ordered_duplicate_detections_500km$Duplicate[i+4]<-"Not Unique"
          }
     }
     
     # then we need to advance the counter and go on to the next cluster
     i<-i+num_in_cluster
}

# last, recombine the unique and not unique data frames
combined_detections_500km<-rbind.data.frame(ordered_duplicate_detections_500km,unique_detections_500km)

# and clean up a bit
rm(ordered_duplicate_detections_500km,duplicate_detections_500km,unique_detections_500km,joined_detections_500km)

# then we can decide what to do with the duplicates
# but first write out to a csv
write.csv(combined_detections_500km,file="/Users/laurenhendricks/Documents/Borneo/Ketapang_ClimateFire/mcd14ml_KTG_500km_dupsID.txt")
```

Plot the detections again, but now color it by whether or not they are duplicated. 
```{r}
#plot(combined_detections_500km$YYYYMMDD,combined_detections_500km$)
```

